{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saminho44/dotfiles/blob/master/Your_first_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfxkQQPnTVhN"
      },
      "source": [
        "## Introduction to HuggingFace's Transformers Library ü§ó\n",
        "\n",
        "\n",
        "To recap, HuggingFace is an AI company that has blown up in the last few years, especially in the realm of Natural Language Processing (NLP).\n",
        "\n",
        "In particular, the Transformers library has revolutionized the way people  work with large-scale transformer models. The goal of this challenge is to introduce you to these models for the first time and show how easy they can be to work with.\n",
        "\n",
        "### Why you should love HuggingFace üåü:\n",
        "\n",
        "#### Pre-trained Models üìö:\n",
        "\n",
        "One of the best features of the Transformers library is its huge repo of pre-trained models. Whether you're looking to employ BERT, GPT-2, T5, RoBERTa, or any of the other transformer architectures, chances are you'll find a version that suits your needs in their model hub.\n",
        "\n",
        "#### It's super easy üëç:\n",
        "\n",
        "The library is designed to be user-friendly. Loading a model and its corresponding tokenizer can be done in just a couple of lines of code. This simplicity extends to fine-tuning as well, allowing you to adapt these powerful models to a wide range of tasks. The `pipelines` library we'll be using lets you go from model selection to getting results in just a few lines.\n",
        "\n",
        "#### Tokenizer  üîÑ and Datasets üìä Library:\n",
        "\n",
        "Alongside the Transformers library, HuggingFace also offers the Tokenizers and Datasets libraries. While the first provides efficient and easy-to-use tokenization methods, the second offers a whole bunch of datasets, meaning you have all the tools and data you need in one ecosystem.\n",
        "\n",
        "#### Community-Driven üåê:\n",
        "The HuggingFace community is very active and any community member (you included) can upload their own models and datasets."
      ],
      "id": "mfxkQQPnTVhN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdZocrL4TVhQ"
      },
      "source": [
        "Enough pre-amble, let's get started by installing the HuggingFace library!"
      ],
      "id": "NdZocrL4TVhQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c-hVGn6TVhQ",
        "outputId": "77327537-684e-4341-ca9c-23db5fecfbd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Install the transformers library from HuggingFace\n",
        "!pip install transformers torch pytesseract"
      ],
      "id": "6c-hVGn6TVhQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "686O10jCTVhR",
        "outputId": "f4252150-181a-4746-8aba-ee67c2b92657"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.0.53)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "# You'll also need some extra tools that some of these models use under the hood\n",
        "! pip install sentencepiece sacremoses"
      ],
      "id": "686O10jCTVhR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGwf0W_WTVhS"
      },
      "source": [
        "Over the course of this notebook, you'll be using Pipelines to download and easily use some very powerful models. Bear in mind that some of these models are quite large (up to 500Mb so make sure you have some disk space free on your machine!). We are going to be using pre-built models and the best resource for implementing them will be using the [Pipelines documentation](https://huggingface.co/docs/transformers/main_classes/pipelines). If you ever want to delete the models after use, you can find them here in your root directory at:\n",
        "\n",
        "`/.cache/huggingface/hub`"
      ],
      "id": "MGwf0W_WTVhS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxQUf54hTVhS"
      },
      "source": [
        "### Basic Sentiment : üòÄ /  üòï / üò† / üòü"
      ],
      "id": "vxQUf54hTVhS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziDCM3V6TVhT"
      },
      "source": [
        "With that in mind, instantiate a pipeline for sentiment analysis __without__ specifying a model and try testing out that model with the sentence \"Transformers are awesome!\" Feel free to try some other sentences, too."
      ],
      "id": "ziDCM3V6TVhT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWhhfxOGTVhT",
        "outputId": "ddaeb63d-173c-40e4-a453-627837ae25e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9998667240142822}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "pass  # YOUR CODE HERE\n",
        "from transformers import pipeline\n",
        "\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "result = sentiment_analysis([\"Transformers are awesome!\"])\n",
        "\n",
        "result"
      ],
      "id": "JWhhfxOGTVhT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDERODAgTVhT"
      },
      "source": [
        "### Nuanced Sentiment ü§î"
      ],
      "id": "PDERODAgTVhT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCYrayhYTVhU"
      },
      "source": [
        "HuggingFace will default to using `distilbert-base-uncased-finetuned-sst-2-english` if we don't specify a model. This model will work fine on a lot of basic use cases, but - because it's been trained on a fairly limited corpus of text:\n",
        "\n",
        "`The Stanford Sentiment Treebank is a corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges.`\n",
        "\n",
        "It's fairly obvious that a model trained on this will likely perform poorly on sentences that include modern language: e.g. \"\"These jokes were absolutely killer!\" or \"These beats are sick!\". Try running these sentences through your pipeline now and you should get negative scores even though they are expressing quite positive sentiment."
      ],
      "id": "NCYrayhYTVhU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA7in98fTVhU",
        "outputId": "966d0d8b-bb7e-4dc0-cca8-ee2286e7f0ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9997040629386902}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "pass  # YOUR CODE HERE\n",
        "result2 = sentiment_analysis([\"These beats are sick!\"])\n",
        "result2"
      ],
      "id": "hA7in98fTVhU"
    },
    {
      "cell_type": "code",
      "source": [
        "result3 = sentiment_analysis([\"These jokes were absolutely killer!\"])\n",
        "result3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XYxHACQWSao",
        "outputId": "d14e1440-8728-4b01-add9-6867cc4961cc"
      },
      "id": "4XYxHACQWSao",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9855958223342896}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sl0B3LbTVhU"
      },
      "source": [
        "Go to the list of HuggingFace models to see if you can find a model that will specialize on Twitter sentiment - hopefully that should be a bit more up to date with all this new lingo! Now create a second pipeline, this time __specifying__ that model that we want to use (use `model=`) and see how our performance instantly improves now we're using a fine-tuned model."
      ],
      "id": "3sl0B3LbTVhU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPX_3FFcTVhU",
        "outputId": "5a6c3eb6-415d-453a-9c9a-329d7cf9bb03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'positive', 'score': 0.7884202003479004}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "pass  # YOUR CODE HERE\n",
        "model2 = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "\n",
        "\n",
        "result4 = model2([\"These beats are sick!\"])\n",
        "\n",
        "result4"
      ],
      "id": "xPX_3FFcTVhU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8fXynkLTVhU"
      },
      "source": [
        "You should see a much more accurate interpretation of the sentiment we're trying to express."
      ],
      "id": "G8fXynkLTVhU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epoVvcGoTVhV"
      },
      "source": [
        "### Sentiment in other languages"
      ],
      "id": "epoVvcGoTVhV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6d5q7uITVhV"
      },
      "source": [
        "While even our first pipeline will actually perform surprisingly well on simple sentences in other languages (e.g. \"C' est bon\" or \"Esta bueno\"), it breaks down when handling more sophisticated ideas in those languages."
      ],
      "id": "N6d5q7uITVhV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoMMT3BITVhV"
      },
      "source": [
        "Here is an example review for the Jurassic World Dominion movie üò¨:\n",
        "\n",
        "\"This was frankly a spectacular failure from start to finish, with  remarkably uninspired performances from some very well-paid actors who acted with all the passion of a wet biscuit\""
      ],
      "id": "OoMMT3BITVhV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhMI7HlwTVhV"
      },
      "source": [
        "Tranlated into Korean it reads as this: \"Ïù¥Í≤ÉÏùÄ ÏÜîÏßÅÌûà Ï≤òÏùåÎ∂ÄÌÑ∞ ÎÅùÍπåÏßÄ ÏóÑÏ≤≠ÎÇú Ïã§Ìå®ÏòÄÏúºÎ©∞ Ï†ñÏùÄ ÎπÑÏä§ÌÇ∑Ïùò Î™®Îì† Ïó¥Ï†ïÏúºÎ°ú Ïó∞Í∏∞Ìïú ÏùºÎ∂Ä Îß§Ïö∞ Î≥¥ÏàòÍ∞Ä Ï¢ãÏùÄ Î∞∞Ïö∞Îì§Ïùò ÌòÑÏ†ÄÌïòÍ≤å ÏòÅÍ∞êÏùÑ Î∞õÏßÄ Î™ªÌïú Ïó∞Í∏∞Î°ú ÎÅùÎÇ¨ÏäµÎãàÎã§.\""
      ],
      "id": "hhMI7HlwTVhV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fr2wLcfTVhV"
      },
      "source": [
        "Try running the Korean text through either your Twitter model or original sentiment mode; you should see they won't pick up on how bad the review is."
      ],
      "id": "5fr2wLcfTVhV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt1-0O5oTVhV",
        "outputId": "7c725c81-c8fd-4fbd-d3b4-fd23f9d0243c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'neutral', 'score': 0.7584187388420105}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "pass  # YOUR CODE HERE\n",
        "result5 = model2([\"Ïù¥Í≤ÉÏùÄ ÏÜîÏßÅÌûà Ï≤òÏùåÎ∂ÄÌÑ∞ ÎÅùÍπåÏßÄ ÏóÑÏ≤≠ÎÇú Ïã§Ìå®ÏòÄÏúºÎ©∞ Ï†ñÏùÄ ÎπÑÏä§ÌÇ∑Ïùò Î™®Îì† Ïó¥Ï†ïÏúºÎ°ú Ïó∞Í∏∞Ìïú ÏùºÎ∂Ä Îß§Ïö∞ Î≥¥ÏàòÍ∞Ä Ï¢ãÏùÄ Î∞∞Ïö∞Îì§Ïùò ÌòÑÏ†ÄÌïòÍ≤å ÏòÅÍ∞êÏùÑ Î∞õÏßÄ Î™ªÌïú Ïó∞Í∏∞Î°ú ÎÅùÎÇ¨ÏäµÎãàÎã§.\"])\n",
        "result5"
      ],
      "id": "vt1-0O5oTVhV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fcHgt_4TVhV"
      },
      "source": [
        "Now see if you can find a model that might perform better in the HuggingFace library and use it."
      ],
      "id": "5fcHgt_4TVhV"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZCpbCswgj8FE"
      },
      "id": "ZCpbCswgj8FE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhtnYbD_TVhV"
      },
      "outputs": [],
      "source": [
        "pass  # YOUR CODE HERE\n",
        "model3 = pipeline(\"sentiment-analysis\", model=\"matthewburke/korean_sentiment\")"
      ],
      "id": "qhtnYbD_TVhV"
    },
    {
      "cell_type": "code",
      "source": [
        "result6 = model3([\"Ïù¥Í≤ÉÏùÄ ÏÜîÏßÅÌûà Ï≤òÏùåÎ∂ÄÌÑ∞ ÎÅùÍπåÏßÄ ÏóÑÏ≤≠ÎÇú Ïã§Ìå®ÏòÄÏúºÎ©∞ Ï†ñÏùÄ ÎπÑÏä§ÌÇ∑Ïùò Î™®Îì† Ïó¥Ï†ïÏúºÎ°ú Ïó∞Í∏∞Ìïú ÏùºÎ∂Ä Îß§Ïö∞ Î≥¥ÏàòÍ∞Ä Ï¢ãÏùÄ Î∞∞Ïö∞Îì§Ïùò ÌòÑÏ†ÄÌïòÍ≤å ÏòÅÍ∞êÏùÑ Î∞õÏßÄ Î™ªÌïú Ïó∞Í∏∞Î°ú ÎÅùÎÇ¨ÏäµÎãàÎã§.\"])\n",
        "result6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9gOxXwmbJ9W",
        "outputId": "90a94264-c2bf-4837-e5e6-3472f7628c75"
      },
      "id": "u9gOxXwmbJ9W",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'LABEL_0', 'score': 0.9615505337715149}]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPLVosdjTVhV"
      },
      "source": [
        "### Translation ‚úçÔ∏è"
      ],
      "id": "EPLVosdjTVhV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKrRywVrTVhW"
      },
      "source": [
        "Let's stick with our language theme and see if we can find a model that can handle the tasks of translating some sentences for us. The `opus-mt` project from the University of Helsinki is incredibly active on HuggingFace, creating and maintaining models designed to democratize the translation process for many different global languages. Try to find one that will allow you to translate from English to Spanishgfjt f"
      ],
      "id": "iKrRywVrTVhW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zg_9KdSkTVhW",
        "outputId": "42a02917-4575-4bc1-b793-ae4c86001bc9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'translation_text': 'Te quiero.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "pass  # YOUR CODE HERE\n",
        "\n",
        "model5 = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
        "result7 = model5('I love you')\n",
        "result7"
      ],
      "id": "zg_9KdSkTVhW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ81u1DGTVhW"
      },
      "source": [
        "### Summarization"
      ],
      "id": "eZ81u1DGTVhW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9072gV2gTVhW"
      },
      "source": [
        "Another really useful NLP task is summarizing a large amount of information into a very small amount of words. BART is a model that performs well on tasks like summarization; it contains a combination of two models you've already seen briefly in the lecture the - the BERT model and and autogressive style GPT model - check out this [link](https://www.projectpro.io/article/transformers-bart-model-explained/553) for some more information on it.\n",
        "\n",
        "Since BART models can be quite large, try to find the `distilbart-xsum-12-6` model on HuggingFace which is one of the smallest distillations available (we'll talk more about distillations later!). Integrate that model into a `\"summarization\"` pipeline, then try to scrape your own article from [the BBC](https://www.bbc.com/news/topics/cx2pk70323et) and summarize it with your pipeline!"
      ],
      "id": "9072gV2gTVhW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bb1i-WSTVhW"
      },
      "outputs": [],
      "source": [
        "pass  # YOUR CODE HERE\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "def summarize_bbc_article(url):\n",
        "\n",
        "\n",
        "  # Scrape the article text\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "  article_text = soup.find(\"div\", class_=\"story-body\")\n",
        "\n",
        "  # Check if the article text is not None\n",
        "  if article_text is not None:\n",
        "    # Load the distilbart-xsum-12-6 model\n",
        "    model = (\"sshleifer/distilbart-xsum-12-6\")\n",
        "\n",
        "\n",
        "  summarization_pipeline = sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "result = sentiment_analysis([\"Transformers are awesome!\"])\n",
        "\n",
        "result\n",
        "\n",
        "    # Print the summary\n",
        "    print(tokenizer.decode(summary[0]))\n"
      ],
      "id": "5bb1i-WSTVhW"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YEegF9zekELu"
      },
      "id": "YEegF9zekELu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(summarize_bbc_article(\"https://www.bbc.com/news/uk-england-hereford-worcester-66279315\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3EReD0fiz6A",
        "outputId": "cf351b07-f88f-4195-b9be-22b4662a5cde"
      },
      "id": "H3EReD0fiz6A",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwnTFaHTTVhW"
      },
      "source": [
        "Once you've done it for one article, try building a function that scrapes BBC urls and summarizes them."
      ],
      "id": "DwnTFaHTTVhW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cn6HdpABTVhW"
      },
      "outputs": [],
      "source": [
        "pass  # YOUR CODE HERE\n"
      ],
      "id": "Cn6HdpABTVhW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "kodUXOgDTVhW",
        "outputId": "e1927aa0-36e8-4d15-b270-6e00fe4ec841"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-8e400291a3c7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummarize_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.bbc.com/news/uk-england-hereford-worcester-66279315\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'summarize_url' is not defined"
          ]
        }
      ],
      "source": [
        "summarize_url(\"https://www.bbc.com/news/uk-england-hereford-worcester-66279315\")"
      ],
      "id": "kodUXOgDTVhW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It70CeNlTVhX"
      },
      "source": [
        "### Going further: Question Answering üîç"
      ],
      "id": "It70CeNlTVhX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOcM0QntTVhX"
      },
      "source": [
        "What if we wanted to go further than just a summary? Perhaps asking questions about a specific dataset in an intuitive way? There's a model for that, too! Enter the (reasonably small) `roberta-base-squad2` - a model trained on question-answer pairs that can answer a `question` about a provided `context` (a body of text you will provide). Check the docs [here](https://huggingface.co/deepset/roberta-base-squad2?context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species.&question=How+many+species+are+in+the+Amazon%3F)."
      ],
      "id": "TOcM0QntTVhX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ8-2JhsTVhX"
      },
      "source": [
        "You know the drill: Create a `\"question-answering\"` pipeline with the `roberta-base-squad2` model, then try putting the `article` you picked before as your context and try asking a `question` about it."
      ],
      "id": "QZ8-2JhsTVhX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPa2j2uFTVhX"
      },
      "outputs": [],
      "source": [
        "pass  # YOUR CODE HERE"
      ],
      "id": "JPa2j2uFTVhX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHqUXPJbTVhX"
      },
      "source": [
        "### Speech to text üé§"
      ],
      "id": "dHqUXPJbTVhX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQKWiWbBTVhX"
      },
      "source": [
        "One of the best models for converting speech to text was made is the open source Whisper model made by OpenAI (creator of ChatGPT etc.) Take a look at the diagram of the model architecture - it should now look quite similar to those you've already seen today:\n",
        "\n",
        "\n",
        "<img src = https://wagon-public-datasets.s3.amazonaws.com/data-science-images/lectures/Transformers/whipser.png width = 450px>"
      ],
      "id": "tQKWiWbBTVhX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ed9XsRNTVhX"
      },
      "source": [
        "Run the following command to download this audio sample and install some additional required packages:"
      ],
      "id": "4ed9XsRNTVhX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiLYSjePTVhX"
      },
      "outputs": [],
      "source": [
        "Uncomment line below for Windows/ Linux\n",
        "#!sudo apt install ffmpeg\n",
        "\n",
        "Uncomment line below for Mac users\n",
        "#!HOMEBREW_NO_AUTO_UPDATE=1 brew install ffmpeg\n",
        "\n",
        "!mkdir data\n",
        "!curl https://wagon-public-datasets.s3.amazonaws.com/deep_learning_datasets/harvard.wav > data/harvard.wav"
      ],
      "id": "iiLYSjePTVhX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGaXeTpVTVhe"
      },
      "source": [
        "You can listen to the clip by using the by importing `IPython` and loading the audio file (see the Algebra day recap for an example of how this is done!)"
      ],
      "id": "AGaXeTpVTVhe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Co__T9kwTVhe"
      },
      "outputs": [],
      "source": [
        "pass  # YOUR CODE HERE"
      ],
      "id": "Co__T9kwTVhe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDll8jQ4TVhe"
      },
      "source": [
        "Find the smallest Whisper model version on HuggingFace and use it to transcribe the audio. Try it on some other `.wav` files if you'd like!"
      ],
      "id": "vDll8jQ4TVhe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZoF6FdvTVhe"
      },
      "outputs": [],
      "source": [
        "pass  # YOUR CODE HERE"
      ],
      "id": "tZoF6FdvTVhe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa4cyjlLTVhe"
      },
      "source": [
        "### Let's get multimodal üòé: Visual Question Answering"
      ],
      "id": "xa4cyjlLTVhe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ikiFUdTTVhe"
      },
      "source": [
        "We can even use question-answering style models on images if we'd like. Many of these models will use chains under the hood that will extract text from an image then pass it through to a language model. In order to use the following model you will need to make sure you `pip install Pillow pytesseract` which are two libraries that will help us to extract text from our images.\n",
        "\n",
        "Once that's done, we're going to create a `\"document-question-answering\"` pipeline - we'll need a model for it, so search for the `layoutlm-invoices` model on HuggingFace. Then try to ask questions about this [`receipt.webp`](https://wagon-public-datasets.s3.amazonaws.com/data-science-images/lectures/Transformers/receipt.webp) (you'll need to download it first). Try asking how much the eggs cost, what sales tax was and what the total was. Feel free to try it on some of your own images!"
      ],
      "id": "5ikiFUdTTVhe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSJoHEo-TVhe"
      },
      "source": [
        "For this to run, you'll need some dependencies:"
      ],
      "id": "lSJoHEo-TVhe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NROnlCZUTVhf"
      },
      "outputs": [],
      "source": [
        "For Mac, uncomment:\n",
        "#!brew install tesseract\n",
        "\n",
        "For Linux etc. uncomment these:\n",
        "#!sudo apt install tesseract-ocr\n",
        "#!sudo apt install libtesseract-dev\n",
        "\n",
        "# Then restart your kernel and give it a try!"
      ],
      "id": "NROnlCZUTVhf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OopX5ueVTVhf"
      },
      "outputs": [],
      "source": [
        "pass  # YOUR CODE HERE"
      ],
      "id": "OopX5ueVTVhf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r-Gx_dmTVhf"
      },
      "source": [
        "Congrats üéâ You've just seen how simple it can be to start working with some advanced Transformer-based models and we've only just scratched the surface.\n",
        "\n",
        "There are so many models you can explore in the HuggingFace library for all kinds of different tasks. Your imagination is literally the limit (well - your compute power can be a limit somtimes üòÖ). To take these models even further for custom usage, we're going to tackle fine-tuning next."
      ],
      "id": "7r-Gx_dmTVhf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiTsZSCZTVhf"
      },
      "source": [
        "‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è Don't forget to clean up your `/.cache/huggingface/hub` if you're limited on space or you'll have a lot of unwanted models hanging around in your cache üßπ ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è"
      ],
      "id": "yiTsZSCZTVhf"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}